/**
 * åŒæœŸã‚µãƒ¼ãƒ“ã‚¹
 *
 * GitHub ã‹ã‚‰ Power Platform ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæƒ…å ±ã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
 * å·®åˆ†åŒæœŸ: SHA ã‚’æ¯”è¼ƒã—ã¦å¤‰æ›´ãŒã‚ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ fetch
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸ: æ¤œç´¢æ™‚ã«éåŒæœŸã§åŒæœŸã‚’å®Ÿè¡Œ
 */

import type { Database as SqlJsDatabase } from "sql.js";
import type { SyncResult } from "../types.js";
import { saveDatabase } from "../database/database.js";
import {
  getWhatsNewFiles,
  fetchAndParseFile,
  getRecentCommits,
  getRecentlyChangedFiles,
  getChangedFilesSince,
  getAllRepositoryLatestShas,
} from "../api/githubClient.js";
import {
  upsertUpdate,
  upsertCommit,
  updateSyncCheckpoint,
  getSyncCheckpoint,
  updateCommitDate,
  getFileShaMap,
  getAllRepoShas,
  upsertRepoSha,
} from "../database/queries.js";
import * as logger from "../utils/logger.js";

/** ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸã®çŠ¶æ…‹ */
let backgroundSyncPromise: Promise<SyncResult> | null = null;
let backgroundSyncDb: SqlJsDatabase | null = null;

/**
 * åŒæœŸã‚ªãƒ—ã‚·ãƒ§ãƒ³
 */
export interface SyncOptions {
  /** GitHub ãƒˆãƒ¼ã‚¯ãƒ³ */
  token?: string;
  /** å¼·åˆ¶åŒæœŸï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡è¦–ï¼‰ */
  force?: boolean;
  /** æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•° */
  maxFiles?: number;
  /** ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«åŒæœŸï¼ˆå·®åˆ†ã®ã¿ï¼‰ */
  incremental?: boolean;
}

/**
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
 */
export function needsBackgroundSync(
  db: SqlJsDatabase,
  staleHours: number = 1,
): boolean {
  try {
    const checkpoint = getSyncCheckpoint(db);
    if (!checkpoint.lastSync) return true;

    const lastSyncDate = new Date(checkpoint.lastSync);
    const hoursSinceLastSync =
      (Date.now() - lastSyncDate.getTime()) / (1000 * 60 * 60);
    return hoursSinceLastSync >= staleHours;
  } catch {
    return true;
  }
}

/**
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸã‚’é–‹å§‹ï¼ˆéãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
 */
export function startBackgroundSync(
  db: SqlJsDatabase,
  options: SyncOptions = {},
): void {
  // ã™ã§ã«åŒæœŸä¸­ãªã‚‰ä½•ã‚‚ã—ãªã„
  if (backgroundSyncPromise && backgroundSyncDb === db) {
    logger.info("Background sync already in progress, skipping");
    return;
  }

  logger.info("Starting background sync");
  backgroundSyncDb = db;
  backgroundSyncPromise = syncFromGitHub(db, { ...options, incremental: true })
    .then((result) => {
      logger.info("Background sync completed", {
        durationMs: result.durationMs,
      });
      return result;
    })
    .catch((error) => {
      logger.error("Background sync failed", { error: String(error) });
      return {
        success: false,
        updatesCount: 0,
        commitsCount: 0,
        durationMs: 0,
        error: String(error),
      };
    })
    .finally(() => {
      backgroundSyncPromise = null;
      backgroundSyncDb = null;
    });
}

/**
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸã®çŠ¶æ…‹ã‚’å–å¾—
 */
export function isBackgroundSyncRunning(): boolean {
  return backgroundSyncPromise !== null;
}

/**
 * åŒæœŸå®Ÿè¡Œ
 */
export async function syncFromGitHub(
  db: SqlJsDatabase,
  options: SyncOptions = {},
): Promise<SyncResult> {
  const startTime = Date.now();
  const { token, force = false, maxFiles = 500 } = options;

  try {
    // åŒæœŸçŠ¶æ…‹ã‚’æ›´æ–°
    updateSyncCheckpoint(db, { syncStatus: "syncing", lastError: null });

    // å‰å›åŒæœŸã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
    const checkpoint = getSyncCheckpoint(db);
    const lastSyncDate = new Date(checkpoint.lastSync);
    const hoursSinceLastSync =
      (Date.now() - lastSyncDate.getTime()) / (1000 * 60 * 60);

    if (!force && hoursSinceLastSync < 1) {
      logger.info("Skipping sync, last sync was recent", {
        hoursSinceLastSync,
      });
      updateSyncCheckpoint(db, { syncStatus: "idle" });
      return {
        success: true,
        updatesCount: checkpoint.recordCount,
        commitsCount: 0,
        durationMs: Date.now() - startTime,
      };
    }

    const { incremental = false } = options;

    logger.info("Starting sync from GitHub", {
      force,
      maxFiles,
      hasToken: !!token,
      incremental,
    });

    // ğŸš€ ãƒªãƒã‚¸ãƒˆãƒªãƒ¬ãƒ™ãƒ«å·®åˆ†ãƒã‚§ãƒƒã‚¯ï¼ˆè»½é‡APIï¼‰
    if (!force) {
      const savedRepoShas = getAllRepoShas(db);
      const latestRepoShas = await getAllRepositoryLatestShas(token);

      // å¤‰æ›´ãŒã‚ã£ãŸãƒªãƒã‚¸ãƒˆãƒªã‚’ç‰¹å®š
      const changedRepos: string[] = [];
      for (const [repo, latestSha] of latestRepoShas) {
        const savedSha = savedRepoShas.get(repo);
        if (savedSha !== latestSha) {
          changedRepos.push(repo);
        }
      }

      logger.info("Repository-level diff check", {
        totalRepos: latestRepoShas.size,
        changedRepos: changedRepos.length,
        changed: changedRepos,
      });

      // å¤‰æ›´ãªã— â†’ ã‚¹ã‚­ãƒƒãƒ—
      if (changedRepos.length === 0) {
        logger.info("No repository changes detected, skipping sync");
        updateSyncCheckpoint(db, { syncStatus: "idle" });
        return {
          success: true,
          updatesCount: checkpoint.recordCount,
          commitsCount: 0,
          durationMs: Date.now() - startTime,
        };
      }

      // SHAã‚’ä¿å­˜
      for (const [repo, sha] of latestRepoShas) {
        upsertRepoSha(db, repo, sha);
      }
    }

    // ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«åŒæœŸ: å‰å›åŒæœŸä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—
    if (incremental && !force && checkpoint.lastSync) {
      logger.info("Using incremental sync mode");
      return await incrementalSync(db, checkpoint.lastSync, token);
    }

    // what's-new ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆSHA å«ã‚€ï¼‰
    logger.info("Fetching what's-new files from GitHub...");
    const files = await getWhatsNewFiles(token);
    logger.info("Fetched files from GitHub", { count: files.length });

    // æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã® SHA ãƒãƒƒãƒ—ã‚’å–å¾—
    const existingShaMap = getFileShaMap(db);
    const isFirstSync = existingShaMap.size === 0;

    // å·®åˆ†åŒæœŸ: SHA ãŒå¤‰ã‚ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†
    const filesToProcess = files
      .filter((file) => {
        if (force || isFirstSync) return true;
        const existingSha = existingShaMap.get(file.path);
        return existingSha !== file.sha;
      })
      .slice(0, maxFiles);

    logger.info("Files to process", {
      total: files.length,
      changed: filesToProcess.length,
      isFirstSync,
      force,
    });

    let updatesCount = 0;
    let errorCount = 0;

    // å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†ï¼ˆåŒæ™‚5ä»¶ã¾ã§ï¼‰
    logger.info("Processing files (parallel)...", {
      count: filesToProcess.length,
    });

    const PARALLEL_LIMIT = 5;
    const chunks: (typeof filesToProcess)[] = [];
    for (let i = 0; i < filesToProcess.length; i += PARALLEL_LIMIT) {
      chunks.push(filesToProcess.slice(i, i + PARALLEL_LIMIT));
    }

    for (const chunk of chunks) {
      const results = await Promise.all(
        chunk.map(async (file) => {
          try {
            const update = await fetchAndParseFile(
              file.rawUrl,
              file.path,
              file.repo,
              token,
            );
            // SHA ã‚’ä¿å­˜
            update.commitSha = file.sha;
            return { success: true, update, path: file.path };
          } catch (error) {
            logger.error("Failed to process file", {
              path: file.path,
              rawUrl: file.rawUrl,
              error: error instanceof Error ? error.stack : String(error),
            });
            return { success: false, update: null, path: file.path };
          }
        }),
      );

      // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆç›´åˆ—ã§è¡Œã†ï¼‰
      for (const result of results) {
        if (result.success && result.update) {
          upsertUpdate(db, result.update);
          updatesCount++;
          logger.info("Successfully processed file", {
            path: result.path,
            title: result.update.title,
          });
        } else {
          errorCount++;
        }
      }
    }
    logger.info("File processing complete (parallel)", {
      updatesCount,
      errorCount,
    });

    // ã‚³ãƒŸãƒƒãƒˆå±¥æ­´ã‚’å–å¾—
    const since = force ? undefined : checkpoint.lastSync;
    const commits = await getRecentCommits(since, token);
    let commitsCount = 0;

    for (const commit of commits) {
      try {
        upsertCommit(db, commit);
        commitsCount++;
      } catch (error) {
        logger.warn("Failed to save commit", {
          sha: commit.sha,
          error: String(error),
        });
      }
    }

    // æœ€è¿‘å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒŸãƒƒãƒˆæ—¥ã‚’æ›´æ–°
    const oneWeekAgo = new Date();
    oneWeekAgo.setDate(oneWeekAgo.getDate() - 7);
    const changedFiles = await getRecentlyChangedFiles(
      oneWeekAgo.toISOString(),
      token,
    );

    for (const [filePath, info] of changedFiles) {
      try {
        updateCommitDate(db, filePath, info.date, info.sha);
      } catch (error) {
        logger.warn("Failed to update commit date", {
          filePath,
          error: String(error),
        });
      }
    }

    const durationMs = Date.now() - startTime;

    // åŒæœŸå®Œäº†
    updateSyncCheckpoint(db, {
      lastSync: new Date().toISOString(),
      syncStatus: "idle",
      recordCount: updatesCount,
      lastSyncDurationMs: durationMs,
      lastError: errorCount > 0 ? `${errorCount} files failed` : null,
    });

    // sql.js ã¯ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªDBãªã®ã§æ˜ç¤ºçš„ã«ä¿å­˜
    saveDatabase();

    logger.info("Sync completed", {
      updatesCount,
      commitsCount,
      errorCount,
      durationMs,
    });

    return {
      success: true,
      updatesCount,
      commitsCount,
      durationMs,
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    updateSyncCheckpoint(db, {
      syncStatus: "error",
      lastError: errorMessage,
    });

    logger.error("Sync failed", { error: errorMessage, durationMs });

    return {
      success: false,
      updatesCount: 0,
      commitsCount: 0,
      durationMs,
      error: errorMessage,
    };
  }
}

/**
 * ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«åŒæœŸï¼ˆå·®åˆ†ã®ã¿ï¼‰
 * å‰å›åŒæœŸä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å–å¾—ãƒ»æ›´æ–°
 */
async function incrementalSync(
  db: SqlJsDatabase,
  lastSync: string,
  token?: string,
): Promise<SyncResult> {
  const startTime = Date.now();

  try {
    logger.info("Starting incremental sync", { since: lastSync });

    // å‰å›åŒæœŸä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    const changedFiles = await getChangedFilesSince(lastSync, token);
    logger.info("Found changed files since last sync", {
      count: changedFiles.length,
    });

    if (changedFiles.length === 0) {
      const durationMs = Date.now() - startTime;
      updateSyncCheckpoint(db, {
        lastSync: new Date().toISOString(),
        syncStatus: "idle",
        lastSyncDurationMs: durationMs,
      });
      logger.info("No changes since last sync", { durationMs });
      return {
        success: true,
        updatesCount: 0,
        commitsCount: 0,
        durationMs,
      };
    }

    let updatesCount = 0;
    let errorCount = 0;

    // å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†
    const PARALLEL_LIMIT = 5;
    const chunks: (typeof changedFiles)[] = [];
    for (let i = 0; i < changedFiles.length; i += PARALLEL_LIMIT) {
      chunks.push(changedFiles.slice(i, i + PARALLEL_LIMIT));
    }

    for (const chunk of chunks) {
      const results = await Promise.all(
        chunk.map(async (file) => {
          try {
            const update = await fetchAndParseFile(
              file.rawUrl,
              file.path,
              file.repo,
              token,
            );
            update.commitSha = file.sha;
            update.commitDate = file.commitDate;
            return { success: true, update, path: file.path };
          } catch (error) {
            logger.error("Failed to process file (incremental)", {
              path: file.path,
              error: error instanceof Error ? error.message : String(error),
            });
            return { success: false, update: null, path: file.path };
          }
        }),
      );

      for (const result of results) {
        if (result.success && result.update) {
          upsertUpdate(db, result.update);
          updatesCount++;
        } else {
          errorCount++;
        }
      }
    }

    const durationMs = Date.now() - startTime;

    updateSyncCheckpoint(db, {
      lastSync: new Date().toISOString(),
      syncStatus: "idle",
      lastSyncDurationMs: durationMs,
      lastError: errorCount > 0 ? `${errorCount} files failed` : null,
    });

    // sql.js ã¯ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªDBãªã®ã§æ˜ç¤ºçš„ã«ä¿å­˜
    saveDatabase();

    logger.info("Incremental sync completed", {
      updatesCount,
      errorCount,
      durationMs,
    });

    return {
      success: true,
      updatesCount,
      commitsCount: 0,
      durationMs,
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    updateSyncCheckpoint(db, {
      syncStatus: "error",
      lastError: errorMessage,
    });

    logger.error("Incremental sync failed", { error: errorMessage });

    return {
      success: false,
      updatesCount: 0,
      commitsCount: 0,
      durationMs,
      error: errorMessage,
    };
  }
}
