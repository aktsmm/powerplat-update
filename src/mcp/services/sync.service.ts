/**
 * åŒæœŸã‚µãƒ¼ãƒ“ã‚¹
 *
 * GitHub ã‹ã‚‰ Power Platform ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæƒ…å ±ã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
 * å·®åˆ†åŒæœŸ: SHA ã‚’æ¯”è¼ƒã—ã¦å¤‰æ›´ãŒã‚ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ fetch
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸ: æ¤œç´¢æ™‚ã«éåŒæœŸã§åŒæœŸã‚’å®Ÿè¡Œ
 */

import type { Database as SqlJsDatabase } from "sql.js";
import type { SyncResult } from "../types.js";
import { saveDatabase } from "../database/database.js";
import {
  getWhatsNewFiles,
  fetchAndParseFile,
  getRecentCommits,
  getRecentlyChangedFiles,
  getChangedFilesSince,
  getAllRepositoryLatestShas,
} from "../api/githubClient.js";
import {
  upsertUpdate,
  upsertCommit,
  updateSyncCheckpoint,
  getSyncCheckpoint,
  updateCommitDate,
  getFileShaMap,
  getAllRepoShas,
  upsertRepoSha,
} from "../database/queries.js";
import * as logger from "../utils/logger.js";

/** ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸã®çŠ¶æ…‹ */
let backgroundSyncPromise: Promise<SyncResult> | null = null;
let backgroundSyncDb: SqlJsDatabase | null = null;

function getTotalUpdateCount(db: SqlJsDatabase): number {
  const result = db.exec("SELECT COUNT(*) FROM powerplat_updates");
  if (result.length === 0 || result[0].values.length === 0) {
    return 0;
  }
  return Number(result[0].values[0][0] ?? 0);
}

/**
 * åŒæœŸã‚ªãƒ—ã‚·ãƒ§ãƒ³
 */
export interface SyncOptions {
  /** GitHub ãƒˆãƒ¼ã‚¯ãƒ³ */
  token?: string;
  /** å¼·åˆ¶åŒæœŸï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡è¦–ï¼‰ */
  force?: boolean;
  /** æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•° */
  maxFiles?: number;
  /** ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«åŒæœŸï¼ˆå·®åˆ†ã®ã¿ï¼‰ */
  incremental?: boolean;
}

/**
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
 */
export function needsBackgroundSync(
  db: SqlJsDatabase,
  staleHours: number = 1,
): boolean {
  try {
    const checkpoint = getSyncCheckpoint(db);
    if (!checkpoint.lastSync) return true;

    const lastSyncDate = new Date(checkpoint.lastSync);
    const hoursSinceLastSync =
      (Date.now() - lastSyncDate.getTime()) / (1000 * 60 * 60);
    return hoursSinceLastSync >= staleHours;
  } catch {
    return true;
  }
}

/**
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸã‚’é–‹å§‹ï¼ˆéãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
 */
export function startBackgroundSync(
  db: SqlJsDatabase,
  options: SyncOptions = {},
): void {
  // ã™ã§ã«åŒæœŸä¸­ãªã‚‰ä½•ã‚‚ã—ãªã„
  if (backgroundSyncPromise && backgroundSyncDb === db) {
    logger.info("Background sync already in progress, skipping");
    return;
  }

  logger.info("Starting background sync");
  backgroundSyncDb = db;
  backgroundSyncPromise = syncFromGitHub(db, { ...options, incremental: true })
    .then((result) => {
      logger.info("Background sync completed", {
        durationMs: result.durationMs,
      });
      return result;
    })
    .catch((error) => {
      logger.error("Background sync failed", { error: String(error) });
      return {
        success: false,
        updatesCount: 0,
        commitsCount: 0,
        durationMs: 0,
        error: String(error),
      };
    })
    .finally(() => {
      backgroundSyncPromise = null;
      backgroundSyncDb = null;
    });
}

/**
 * ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰åŒæœŸã®çŠ¶æ…‹ã‚’å–å¾—
 */
export function isBackgroundSyncRunning(): boolean {
  return backgroundSyncPromise !== null;
}

/**
 * åŒæœŸå®Ÿè¡Œ
 */
export async function syncFromGitHub(
  db: SqlJsDatabase,
  options: SyncOptions = {},
): Promise<SyncResult> {
  const startTime = Date.now();
  const { token, force = false, maxFiles = 500 } = options;
  let latestRepoShasToPersist: Map<string, string> | null = null;

  try {
    // åŒæœŸçŠ¶æ…‹ã‚’æ›´æ–°
    updateSyncCheckpoint(db, { syncStatus: "syncing", lastError: null });

    // å‰å›åŒæœŸã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’ãƒã‚§ãƒƒã‚¯
    const checkpoint = getSyncCheckpoint(db);
    const lastSyncDate = new Date(checkpoint.lastSync);
    const hoursSinceLastSync =
      (Date.now() - lastSyncDate.getTime()) / (1000 * 60 * 60);

    if (!force && hoursSinceLastSync < 1) {
      const totalRecordCount = getTotalUpdateCount(db);
      logger.info("Skipping sync, last sync was recent", {
        hoursSinceLastSync,
      });
      updateSyncCheckpoint(db, {
        syncStatus: "idle",
        recordCount: totalRecordCount,
      });
      saveDatabase();
      return {
        success: true,
        updatesCount: 0,
        commitsCount: 0,
        durationMs: Date.now() - startTime,
      };
    }

    const { incremental = false } = options;

    logger.info("Starting sync from GitHub", {
      force,
      maxFiles,
      hasToken: !!token,
      incremental,
    });

    // ğŸš€ ãƒªãƒã‚¸ãƒˆãƒªãƒ¬ãƒ™ãƒ«å·®åˆ†ãƒã‚§ãƒƒã‚¯ï¼ˆè»½é‡APIï¼‰
    if (!force) {
      const savedRepoShas = getAllRepoShas(db);
      let latestRepoShas: Map<string, string> | null = null;

      try {
        latestRepoShas = await getAllRepositoryLatestShas(token);
      } catch (error) {
        logger.warn("Repository diff check failed, falling back to full sync", {
          error: String(error),
          savedRepoCount: savedRepoShas.size,
        });
      }

      if (latestRepoShas) {
        const isRepoDiffReliable =
          latestRepoShas.size > 0 &&
          (savedRepoShas.size === 0 ||
            latestRepoShas.size >= savedRepoShas.size);

        if (isRepoDiffReliable) {
          // å¤‰æ›´ãŒã‚ã£ãŸãƒªãƒã‚¸ãƒˆãƒªã‚’ç‰¹å®š
          const changedRepos: string[] = [];
          for (const [repo, latestSha] of latestRepoShas) {
            const savedSha = savedRepoShas.get(repo);
            if (savedSha !== latestSha) {
              changedRepos.push(repo);
            }
          }

          logger.info("Repository-level diff check", {
            totalRepos: latestRepoShas.size,
            changedRepos: changedRepos.length,
            changed: changedRepos,
          });

          // å¤‰æ›´ãªã— â†’ ã‚¹ã‚­ãƒƒãƒ—
          if (changedRepos.length === 0) {
            const durationMs = Date.now() - startTime;
            const totalRecordCount = getTotalUpdateCount(db);
            logger.info("No repository changes detected, skipping sync");
            updateSyncCheckpoint(db, {
              lastSync: new Date().toISOString(),
              syncStatus: "idle",
              recordCount: totalRecordCount,
              lastSyncDurationMs: durationMs,
              lastError: null,
            });
            saveDatabase();
            return {
              success: true,
              updatesCount: 0,
              commitsCount: 0,
              durationMs,
            };
          }

          // åŒæœŸæˆåŠŸæ™‚ã«ã®ã¿ä¿å­˜ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä¿æŒã®ã¿
          latestRepoShasToPersist = latestRepoShas;
        } else {
          logger.warn(
            "Repository diff check incomplete, falling back to full sync",
            {
              savedRepoCount: savedRepoShas.size,
              latestRepoCount: latestRepoShas.size,
            },
          );
        }
      }
    }

    // ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«åŒæœŸ: å‰å›åŒæœŸä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—
    if (incremental && !force && checkpoint.lastSync) {
      logger.info("Using incremental sync mode");
      const result = await incrementalSync(db, checkpoint.lastSync, token);

      if (result.success && latestRepoShasToPersist) {
        for (const [repo, sha] of latestRepoShasToPersist) {
          upsertRepoSha(db, repo, sha);
        }
        saveDatabase();
      }

      return result;
    }

    // what's-new ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—ï¼ˆSHA å«ã‚€ï¼‰
    logger.info("Fetching what's-new files from GitHub...");
    const files = await getWhatsNewFiles(token);
    logger.info("Fetched files from GitHub", { count: files.length });

    // æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã® SHA ãƒãƒƒãƒ—ã‚’å–å¾—
    const existingShaMap = getFileShaMap(db);
    const isFirstSync = existingShaMap.size === 0;

    // å·®åˆ†åŒæœŸ: SHA ãŒå¤‰ã‚ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†
    const filesToProcess = files
      .filter((file) => {
        if (force || isFirstSync) return true;
        const existingSha = existingShaMap.get(file.path);
        return existingSha !== file.sha;
      })
      .slice(0, maxFiles);
    const deferredByMaxFiles = Math.max(files.length - filesToProcess.length, 0);
    const isCappedByMaxFiles = deferredByMaxFiles > 0;

    logger.info("Files to process", {
      total: files.length,
      changed: filesToProcess.length,
      deferredByMaxFiles,
      isFirstSync,
      force,
    });

    let updatesCount = 0;
    let errorCount = 0;

    // å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†ï¼ˆåŒæ™‚5ä»¶ã¾ã§ï¼‰
    logger.info("Processing files (parallel)...", {
      count: filesToProcess.length,
    });

    const PARALLEL_LIMIT = 5;
    const chunks: (typeof filesToProcess)[] = [];
    for (let i = 0; i < filesToProcess.length; i += PARALLEL_LIMIT) {
      chunks.push(filesToProcess.slice(i, i + PARALLEL_LIMIT));
    }

    for (const chunk of chunks) {
      const results = await Promise.all(
        chunk.map(async (file) => {
          try {
            const update = await fetchAndParseFile(
              file.rawUrl,
              file.path,
              file.repo,
              token,
            );
            // SHA ã‚’ä¿å­˜
            update.commitSha = file.sha;
            return { success: true, update, path: file.path };
          } catch (error) {
            logger.error("Failed to process file", {
              path: file.path,
              rawUrl: file.rawUrl,
              error: error instanceof Error ? error.stack : String(error),
            });
            return { success: false, update: null, path: file.path };
          }
        }),
      );

      // ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆç›´åˆ—ã§è¡Œã†ï¼‰
      for (const result of results) {
        if (result.success && result.update) {
          upsertUpdate(db, result.update);
          updatesCount++;
          logger.info("Successfully processed file", {
            path: result.path,
            title: result.update.title,
          });
        } else {
          errorCount++;
        }
      }
    }
    logger.info("File processing complete (parallel)", {
      updatesCount,
      errorCount,
    });

    // ã‚³ãƒŸãƒƒãƒˆå±¥æ­´ã‚’å–å¾—
    const since = force ? undefined : checkpoint.lastSync;
    const commits = await getRecentCommits(since, token);
    let commitsCount = 0;

    for (const commit of commits) {
      try {
        upsertCommit(db, commit);
        commitsCount++;
      } catch (error) {
        logger.warn("Failed to save commit", {
          sha: commit.sha,
          error: String(error),
        });
      }
    }

    // æœ€è¿‘å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚³ãƒŸãƒƒãƒˆæ—¥ã‚’æ›´æ–°
    const oneWeekAgo = new Date();
    oneWeekAgo.setDate(oneWeekAgo.getDate() - 7);
    const changedFiles = await getRecentlyChangedFiles(
      oneWeekAgo.toISOString(),
      token,
    );

    for (const [filePath, info] of changedFiles) {
      try {
        updateCommitDate(db, filePath, info.date, info.sha);
      } catch (error) {
        logger.warn("Failed to update commit date", {
          filePath,
          error: String(error),
        });
      }
    }

    const durationMs = Date.now() - startTime;
    const totalRecordCount = getTotalUpdateCount(db);
    const hasProcessingErrors = errorCount > 0 || isCappedByMaxFiles;
    const processingIssues: string[] = [];
    if (errorCount > 0) {
      processingIssues.push(`${errorCount} files failed`);
    }
    if (isCappedByMaxFiles) {
      processingIssues.push(
        `${deferredByMaxFiles} files deferred by maxFiles limit`,
      );
    }

    if (latestRepoShasToPersist && !hasProcessingErrors) {
      for (const [repo, sha] of latestRepoShasToPersist) {
        upsertRepoSha(db, repo, sha);
      }
    }

    // åŒæœŸå®Œäº†
    const checkpointUpdate: Parameters<typeof updateSyncCheckpoint>[1] = {
      syncStatus: "idle",
      recordCount: totalRecordCount,
      lastSyncDurationMs: durationMs,
      lastError: processingIssues.length > 0 ? processingIssues.join("; ") : null,
    };
    if (!hasProcessingErrors) {
      checkpointUpdate.lastSync = new Date().toISOString();
    }
    updateSyncCheckpoint(db, checkpointUpdate);

    // sql.js ã¯ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªDBãªã®ã§æ˜ç¤ºçš„ã«ä¿å­˜
    saveDatabase();

    logger.info("Sync completed", {
      updatesCount,
      commitsCount,
      errorCount,
      durationMs,
      success: !hasProcessingErrors,
    });

    return {
      success: !hasProcessingErrors,
      updatesCount,
      commitsCount,
      durationMs,
      error: hasProcessingErrors ? processingIssues.join("; ") : undefined,
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    updateSyncCheckpoint(db, {
      syncStatus: "error",
      lastError: errorMessage,
    });
    saveDatabase();

    logger.error("Sync failed", { error: errorMessage, durationMs });

    return {
      success: false,
      updatesCount: 0,
      commitsCount: 0,
      durationMs,
      error: errorMessage,
    };
  }
}

/**
 * ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«åŒæœŸï¼ˆå·®åˆ†ã®ã¿ï¼‰
 * å‰å›åŒæœŸä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å–å¾—ãƒ»æ›´æ–°
 */
async function incrementalSync(
  db: SqlJsDatabase,
  lastSync: string,
  token?: string,
): Promise<SyncResult> {
  const startTime = Date.now();

  try {
    logger.info("Starting incremental sync", { since: lastSync });

    // å‰å›åŒæœŸä»¥é™ã«å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    const changedFiles = await getChangedFilesSince(lastSync, token);
    logger.info("Found changed files since last sync", {
      count: changedFiles.length,
    });

    if (changedFiles.length === 0) {
      const durationMs = Date.now() - startTime;
      const totalRecordCount = getTotalUpdateCount(db);
      updateSyncCheckpoint(db, {
        lastSync: new Date().toISOString(),
        syncStatus: "idle",
        recordCount: totalRecordCount,
        lastSyncDurationMs: durationMs,
      });
      saveDatabase();
      logger.info("No changes since last sync", { durationMs });
      return {
        success: true,
        updatesCount: 0,
        commitsCount: 0,
        durationMs,
      };
    }

    let updatesCount = 0;
    let errorCount = 0;

    // å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†
    const PARALLEL_LIMIT = 5;
    const chunks: (typeof changedFiles)[] = [];
    for (let i = 0; i < changedFiles.length; i += PARALLEL_LIMIT) {
      chunks.push(changedFiles.slice(i, i + PARALLEL_LIMIT));
    }

    for (const chunk of chunks) {
      const results = await Promise.all(
        chunk.map(async (file) => {
          try {
            const update = await fetchAndParseFile(
              file.rawUrl,
              file.path,
              file.repo,
              token,
            );
            update.commitSha = file.sha;
            update.commitDate = file.commitDate;
            return { success: true, update, path: file.path };
          } catch (error) {
            logger.error("Failed to process file (incremental)", {
              path: file.path,
              error: error instanceof Error ? error.message : String(error),
            });
            return { success: false, update: null, path: file.path };
          }
        }),
      );

      for (const result of results) {
        if (result.success && result.update) {
          upsertUpdate(db, result.update);
          updatesCount++;
        } else {
          errorCount++;
        }
      }
    }

    const durationMs = Date.now() - startTime;
    const totalRecordCount = getTotalUpdateCount(db);
    const hasProcessingErrors = errorCount > 0;

    const checkpointUpdate: Parameters<typeof updateSyncCheckpoint>[1] = {
      syncStatus: "idle",
      recordCount: totalRecordCount,
      lastSyncDurationMs: durationMs,
      lastError: errorCount > 0 ? `${errorCount} files failed` : null,
    };
    if (!hasProcessingErrors) {
      checkpointUpdate.lastSync = new Date().toISOString();
    }
    updateSyncCheckpoint(db, checkpointUpdate);

    // sql.js ã¯ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªDBãªã®ã§æ˜ç¤ºçš„ã«ä¿å­˜
    saveDatabase();

    logger.info("Incremental sync completed", {
      updatesCount,
      errorCount,
      durationMs,
      success: !hasProcessingErrors,
    });

    return {
      success: !hasProcessingErrors,
      updatesCount,
      commitsCount: 0,
      durationMs,
      error: hasProcessingErrors ? `${errorCount} files failed` : undefined,
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    updateSyncCheckpoint(db, {
      syncStatus: "error",
      lastError: errorMessage,
    });
    saveDatabase();

    logger.error("Incremental sync failed", { error: errorMessage });

    return {
      success: false,
      updatesCount: 0,
      commitsCount: 0,
      durationMs,
      error: errorMessage,
    };
  }
}
